{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:25:39.872235Z",
     "start_time": "2025-06-09T05:25:39.862428Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 34,
   "source": [
    "import numpy as np\n",
    "from typing import Dict, Tuple, List, Union\n",
    "\n",
    "Pos = Tuple[int, int]  # (row, col)\n",
    "\n",
    "class Board:\n",
    "    \"\"\"\n",
    "    Peg-Solitaire (7Ã—7 cross) board.\n",
    "    - Peg  : 1\n",
    "    - Hole : 0\n",
    "    - Illegal cell : 0 (×‘×ª×¦×•×’×ª ××¢×¨×š ×‘×œ×‘×“, ××™×Ÿ -1! ×¨××” ×”×¡×‘×¨)\n",
    "    \"\"\"\n",
    "    __slots__ = (\"state\",)\n",
    "\n",
    "    # 33 legal board positions (cross shape)\n",
    "    LEGAL_POSITIONS: List[Pos] = [\n",
    "        (r, c) for r in range(7) for c in range(7)\n",
    "        if (2 <= r <= 4) or (2 <= c <= 4)\n",
    "    ]\n",
    "    LEGAL_MASK: np.ndarray = np.zeros((7, 7), dtype=np.float32)\n",
    "    for _r, _c in LEGAL_POSITIONS:\n",
    "        LEGAL_MASK[_r, _c] = 1.0\n",
    "    TOTAL_PEGS = 32\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Standard start: pegs on all positions, center is empty.\"\"\"\n",
    "        self.state: Dict[Pos, int] = {pos: 1 for pos in self.LEGAL_POSITIONS}\n",
    "        self.state[(3, 3)] = 0\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset to standard board (all pegs, center empty).\"\"\"\n",
    "        for pos in self.LEGAL_POSITIONS:\n",
    "            self.state[pos] = 1\n",
    "        self.state[(3, 3)] = 0\n",
    "\n",
    "    def get(self, pos: Pos) -> Union[int, None]:\n",
    "        \"\"\"Return 1 (peg) / 0 (hole) / None (illegal position).\"\"\"\n",
    "        return self.state.get(pos, None)\n",
    "\n",
    "    def set(self, pos: Pos, value: int) -> None:\n",
    "        \"\"\"Place peg (1) or hole (0) at pos.\"\"\"\n",
    "        if pos not in self.LEGAL_POSITIONS or value not in (0, 1):\n",
    "            raise ValueError(f\"Illegal position/value: {pos} {value}\")\n",
    "        self.state[pos] = value\n",
    "\n",
    "    def all_pegs(self) -> List[Pos]:\n",
    "        return [p for p, v in self.state.items() if v == 1]\n",
    "\n",
    "    def all_holes(self) -> List[Pos]:\n",
    "        return [p for p, v in self.state.items() if v == 0]\n",
    "\n",
    "    def count_pegs(self) -> int:\n",
    "        \"\"\"Return current number of pegs on board.\"\"\"\n",
    "        # ×™×¢×™×œ, ×‘×œ×™ ×§×©×¨ ×œ-numpy, ×¢×•×‘×“ ××•×œ dict\n",
    "        return sum(self.state.values())\n",
    "\n",
    "    def as_array(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Returns a 7Ã—7 array:\n",
    "          1 = peg, 0 = hole or out-of-board\n",
    "        \"\"\"\n",
    "        arr = np.zeros((7, 7), dtype=np.float32)\n",
    "        for pos in self.LEGAL_POSITIONS:\n",
    "            arr[pos] = float(self.state[pos])\n",
    "        return arr\n",
    "\n",
    "    def get_state(self) -> np.ndarray:\n",
    "        \"\"\"Alias for as_array().\"\"\"\n",
    "        return self.as_array()\n",
    "\n",
    "    def set_state(self, data: Union[np.ndarray, Dict[Pos, int]]) -> None:\n",
    "        \"\"\"\n",
    "        Set board from ndarray (7Ã—7) or dict {pos: val}.\n",
    "        Illegal/outside values treated as holes (0).\n",
    "        \"\"\"\n",
    "        if isinstance(data, dict):\n",
    "            for pos in self.LEGAL_POSITIONS:\n",
    "                self.state[pos] = int(data.get(pos, 0))\n",
    "        else:\n",
    "            arr = np.asarray(data)\n",
    "            if arr.shape != (7, 7):\n",
    "                raise ValueError(\"state array must be shape (7,7)\")\n",
    "            for pos in self.LEGAL_POSITIONS:\n",
    "                self.state[pos] = 1 if arr[pos] == 1 else 0\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, d: Dict[Pos, int]) -> \"Board\":\n",
    "        b = cls()\n",
    "        b.set_state(d)\n",
    "        return b\n",
    "\n",
    "    def encode_observation(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Return obs (7,7,4):\n",
    "         ch0: pegs (1/0),\n",
    "         ch1: fraction removed (global),\n",
    "         ch2: fraction remain (global),\n",
    "         ch3: legal mask (1/0)\n",
    "        \"\"\"\n",
    "        arr = self.as_array()\n",
    "        num_pegs = np.sum(arr)\n",
    "        removed = (self.TOTAL_PEGS - num_pegs) / self.TOTAL_PEGS\n",
    "        remain = num_pegs / self.TOTAL_PEGS\n",
    "        obs = np.zeros((7, 7, 4), dtype=np.float32)\n",
    "        obs[:, :, 0] = arr\n",
    "        obs[:, :, 1] = removed\n",
    "        obs[:, :, 2] = remain\n",
    "        obs[:, :, 3] = self.LEGAL_MASK\n",
    "        return obs\n",
    "\n",
    "    @staticmethod\n",
    "    def augment_observation(obs: np.ndarray, mode: str = \"random\") -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Return augmented observation (rot/flip).\n",
    "        mode = 'random' | 'all' | None\n",
    "        \"\"\"\n",
    "        augs = []\n",
    "        for rot in range(4):\n",
    "            r = np.rot90(obs, k=rot, axes=(0, 1))\n",
    "            for flip in (False, True):\n",
    "                augs.append(np.flip(r, axis=1) if flip else r)\n",
    "        if mode == \"all\":\n",
    "            return np.stack(augs)\n",
    "        if mode == \"random\":\n",
    "            idx = np.random.randint(8)\n",
    "            return augs[idx]\n",
    "        return obs\n",
    "\n",
    "    def copy(self) -> \"Board\":\n",
    "        new_b = Board()\n",
    "        new_b.state = self.state.copy()\n",
    "        return new_b\n",
    "\n",
    "    clone = copy  # alias\n",
    "\n",
    "    def to_dict(self) -> Dict[Pos, int]:\n",
    "        return self.state.copy()\n",
    "\n",
    "    def __hash__(self) -> int:\n",
    "        return hash(tuple(sorted(self.state.items())))\n",
    "\n",
    "    def __eq__(self, other: object) -> bool:\n",
    "        return isinstance(other, Board) and self.state == other.state\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        arr = self.as_array()\n",
    "        rows = []\n",
    "        for r in range(7):\n",
    "            rows.append(' '.join('â—' if arr[r, c] == 1 else\n",
    "                                 'â—¯' if arr[r, c] == 0 else\n",
    "                                 ' ' for c in range(7)))\n",
    "        return \"\\n\".join(rows)"
   ],
   "id": "ef45c1ce665cb656"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:25:40.252746Z",
     "start_time": "2025-06-09T05:25:40.242887Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 35,
   "source": [
    "from typing import Tuple, List, Dict, Optional, Callable\n",
    "from copy import deepcopy\n",
    "\n",
    "Pos = Tuple[int, int]\n",
    "Action = Tuple[int, int, int]          # (row, col, dir-idx)\n",
    "\n",
    "\n",
    "class Game:\n",
    "    \"\"\"\n",
    "    Core logic of Peg-Solitaire.\n",
    "    â€¢ Holds a Board instance\n",
    "    â€¢ Generates legal moves / actions\n",
    "    â€¢ Applies moves with full undo/redo support\n",
    "    â€¢ Supplies hooks for RL (reward_fn, clone_state, etc.)\n",
    "    \"\"\"\n",
    "\n",
    "    # Î”-×•×§×˜×•×¨×™× (row,col)   â€“idxâ†’ 0:up  1:down  2:left  3:right\n",
    "    DIRECTIONS: List[Pos] = [(-2, 0), (2, 0), (0, -2), (0, 2)]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        board: Optional[\"Board\"] = None,\n",
    "        reward_fn: Optional[Callable[[\"Board\", Tuple[Pos, Pos, Pos], bool], float]] = None\n",
    "    ) -> None:\n",
    "        self.board: \"Board\" = board.copy() if board else Board()\n",
    "        self.move_history: List[Tuple[Pos, Pos, Pos, \"Board\"]] = []   # (from,to,over, board_before)\n",
    "        self.redo_stack: List[Tuple[Pos, Pos, Pos, \"Board\"]] = []\n",
    "        self.last_move: Optional[Tuple[Pos, Pos, Pos]] = None\n",
    "        self.reward_fn = reward_fn or self._default_reward        # use built-in if None\n",
    "        self.custom_metadata: Dict[str, object] = {}\n",
    "        self.move_log: List[Tuple[Pos, Pos, Pos]] = []\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    #                          move legality                              #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def _middle(self, a: Pos, b: Pos) -> Pos:\n",
    "        return ((a[0] + b[0]) // 2, (a[1] + b[1]) // 2)\n",
    "\n",
    "    def is_legal_move(self, src: Pos, dst: Pos) -> Tuple[bool, Optional[Pos]]:\n",
    "        \"\"\"Return (is_legal, middle_pos) for move srcâ†’dst.\"\"\"\n",
    "        if src not in Board.LEGAL_POSITIONS or dst not in Board.LEGAL_POSITIONS:\n",
    "            return False, None\n",
    "        dr, dc = dst[0] - src[0], dst[1] - src[1]\n",
    "        if (abs(dr), abs(dc)) not in ((2, 0), (0, 2)):\n",
    "            return False, None\n",
    "        over = self._middle(src, dst)\n",
    "        if self.board.get(src) == 1 and self.board.get(over) == 1 and self.board.get(dst) == 0:\n",
    "            return True, over\n",
    "        return False, None\n",
    "\n",
    "    def get_legal_moves(self) -> List[Tuple[Pos, Pos, Pos]]:\n",
    "        \"\"\"List of (src, dst, over) tuples for all legal moves.\"\"\"\n",
    "        moves = []\n",
    "        for src in self.board.all_pegs():\n",
    "            for di, dj in self.DIRECTIONS:\n",
    "                dst = (src[0] + di, src[1] + dj)\n",
    "                legal, over = self.is_legal_move(src, dst)\n",
    "                if legal:\n",
    "                    moves.append((src, dst, over))\n",
    "        return moves\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    #                        RL-friendly actions                          #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def get_legal_actions(self) -> List[Action]:\n",
    "        acts: List[Action] = []\n",
    "        for src in self.board.all_pegs():\n",
    "            for d_idx, (di, dj) in enumerate(self.DIRECTIONS):\n",
    "                dst = (src[0] + di, src[1] + dj)\n",
    "                legal, _ = self.is_legal_move(src, dst)\n",
    "                if legal:\n",
    "                    acts.append((src[0], src[1], d_idx))\n",
    "        return acts\n",
    "\n",
    "    def is_legal_action(self, a: Action) -> bool:\n",
    "        r, c, d = a\n",
    "        dr, dc = self.DIRECTIONS[d]\n",
    "        return self.is_legal_move((r, c), (r + dr, c + dc))[0]\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    #                              apply                                  #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def _apply(self, src: Pos, dst: Pos, over: Pos) -> None:\n",
    "        \"\"\"Mutate board: execute the (already validated) jump.\"\"\"\n",
    "        self.board.set(src, 0)\n",
    "        self.board.set(over, 0)\n",
    "        self.board.set(dst, 1)\n",
    "\n",
    "    def apply_move(self, src: Pos, dst: Pos) -> Tuple[bool, float, bool, Dict]:\n",
    "        legal, over = self.is_legal_move(src, dst)\n",
    "        if not legal:\n",
    "            return False, 0.0, self.is_game_over(), {\"reason\": \"illegal\"}\n",
    "\n",
    "        before = self.board.copy()\n",
    "        self.move_history.append((src, dst, over, before))\n",
    "        self.redo_stack.clear()\n",
    "\n",
    "        self._apply(src, dst, over)\n",
    "        self.last_move = (src, over, dst)\n",
    "        self.move_log.append(self.last_move)\n",
    "\n",
    "        done = self.is_game_over()\n",
    "        reward = self.reward_fn(self.board, self.last_move, done)\n",
    "        return True, reward, done, {\"last_move\": self.last_move, \"done\": done}\n",
    "\n",
    "    def apply_action(self, action: Action) -> Tuple[bool, float, bool, Dict]:\n",
    "        r, c, d = action\n",
    "        dr, dc = self.DIRECTIONS[d]\n",
    "        return self.apply_move((r, c), (r + dr, c + dc))\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    #                          undo / redo                                #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def undo(self) -> bool:\n",
    "        if not self.move_history:\n",
    "            return False\n",
    "        src, dst, over, before = self.move_history.pop()\n",
    "        self.redo_stack.append((src, dst, over, self.board.copy()))\n",
    "        self.board = before\n",
    "        self.last_move = self.move_history[-1][:3] if self.move_history else None\n",
    "        if self.move_log:\n",
    "            self.move_log.pop()\n",
    "        return True\n",
    "\n",
    "    def redo(self) -> bool:\n",
    "        if not self.redo_stack:\n",
    "            return False\n",
    "        src, dst, over, before = self.redo_stack.pop()\n",
    "        self.move_history.append((src, dst, over, before))\n",
    "        self.board = before.copy()\n",
    "        self._apply(src, dst, over)\n",
    "        self.last_move = (src, over, dst)\n",
    "        self.move_log.append(self.last_move)\n",
    "        return True\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    #                         termination checks                          #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def is_game_over(self) -> bool:\n",
    "        return not self.get_legal_moves()\n",
    "\n",
    "    def is_win(self) -> bool:\n",
    "        return self.board.count_pegs() == 1 and self.board.get((3, 3)) == 1\n",
    "\n",
    "    is_solved = is_win  # alias\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    #                             reset / clone                           #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def reset(self, board: Optional[\"Board\"] = None) -> None:\n",
    "        self.board = board.copy() if board else Board()\n",
    "        self.move_history.clear()\n",
    "        self.redo_stack.clear()\n",
    "        self.last_move = None\n",
    "        self.move_log.clear()\n",
    "        self.custom_metadata.clear()\n",
    "\n",
    "    def clone_state(self) -> \"Game\":\n",
    "        \"\"\"Deep copy for MCTS.\"\"\"\n",
    "        return Game(board=self.board.copy(), reward_fn=self.reward_fn)\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    #                       misc / metadata / io                          #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def export_move_log(self) -> List[Tuple[Pos, Pos, Pos]]:\n",
    "        return self.move_log.copy()\n",
    "\n",
    "    def set_state(self, state: Union[\"Board\", Dict[Pos, int]]) -> None:\n",
    "        if isinstance(state, Board):\n",
    "            self.board = state.copy()\n",
    "        elif isinstance(state, dict):\n",
    "            self.board.set_state(state)\n",
    "        else:\n",
    "            raise TypeError(\"state must be Board or dict\")\n",
    "        self.move_history.clear()\n",
    "        self.redo_stack.clear()\n",
    "        self.last_move = None\n",
    "\n",
    "    # custom metadata\n",
    "    def get_custom_metadata(self, key: str, default=None):\n",
    "        return self.custom_metadata.get(key, default)\n",
    "\n",
    "    def set_custom_metadata(self, key: str, val):\n",
    "        self.custom_metadata[key] = val\n",
    "\n",
    "    # hashing / equality\n",
    "    def __hash__(self) -> int:\n",
    "        return hash(self.board)\n",
    "\n",
    "    def __eq__(self, other: object) -> bool:\n",
    "        return isinstance(other, Game) and self.board == other.board\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    #                          reward default                             #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    @staticmethod\n",
    "    def _default_reward(board: \"Board\", last_mv: Tuple[Pos, Pos, Pos], done: bool) -> float:\n",
    "        \"\"\"+1 solved / 0.1 per jump / 0 on fail.\"\"\"\n",
    "        if done and board.count_pegs() == 1:\n",
    "            return 1.0\n",
    "        return 0.1\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    #                           debug print                               #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def __str__(self) -> str:\n",
    "        txt = [str(self.board)]\n",
    "        if self.last_move:\n",
    "            txt.append(f\"Last move: {self.last_move}\")\n",
    "        return \"\\n\".join(txt)"
   ],
   "id": "cc4ff505119f98f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:25:40.640090Z",
     "start_time": "2025-06-09T05:25:40.633508Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 36,
   "source": [
    "from typing import Callable, Optional, Tuple, List\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PegSolitaireEnv:\n",
    "    \"\"\"\n",
    "    Minimal-Gym-like environment for Peg-Solitaire.\n",
    "\n",
    "    Observation shape : (7, 7, 4) â€“ see encode_observation().\n",
    "    Action            : tuple (row, col, dir-idx) â€” dir-idxâˆˆ{0:â†‘,1:â†“,2:â†,3:â†’}\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    board_cls : type[Board]\n",
    "    game_cls  : type[Game]\n",
    "    reward_fn : Optional custom callable overriding Game.reward_fn\n",
    "    \"\"\"\n",
    "\n",
    "    BOARD_SIZE = 7\n",
    "    TOTAL_PEGS = 32\n",
    "\n",
    "    # ---------------------- ctor / reset ---------------------- #\n",
    "    def __init__(\n",
    "        self,\n",
    "        board_cls,\n",
    "        game_cls,\n",
    "        reward_fn: Optional[Callable[[\"Board\", Tuple, bool], float]] = None\n",
    "    ) -> None:\n",
    "        self._board_cls = board_cls\n",
    "        self._game_cls = game_cls\n",
    "        self.game = game_cls(board_cls(), reward_fn=reward_fn)\n",
    "        self.done = False\n",
    "        self.board_mask = self._generate_board_mask()\n",
    "\n",
    "    def _generate_board_mask(self) -> np.ndarray:\n",
    "        mask = np.zeros((self.BOARD_SIZE, self.BOARD_SIZE), dtype=np.float32)\n",
    "        for r in range(7):\n",
    "            for c in range(7):\n",
    "                if (2 <= r <= 4) or (2 <= c <= 4):\n",
    "                    mask[r, c] = 1.0\n",
    "        return mask\n",
    "\n",
    "    # Gym-style API ------------------------------------------------------ #\n",
    "    def reset(self, state=None) -> Tuple[np.ndarray, dict]:\n",
    "        \"\"\"Reset env. Optionally load custom board state (dict|ndarray|Board).\"\"\"\n",
    "        if state is None:\n",
    "            self.game.reset()\n",
    "        else:\n",
    "            self.game.set_state(state)\n",
    "        self.done = False\n",
    "        obs = self.encode_observation()\n",
    "        info = {\"num_pegs\": self.game.board.count_pegs()}\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action: Tuple[int, int, int]) -> Tuple[np.ndarray, float, bool, dict]:\n",
    "        \"\"\"Apply action, return (obs, reward, done, info).\"\"\"\n",
    "        if self.done:\n",
    "            raise RuntimeError(\"Call reset() before further step()s.\")\n",
    "\n",
    "        if self.game.is_legal_action(action):\n",
    "            _, reward, self.done, _ = self.game.apply_action(action)\n",
    "        else:\n",
    "            reward = -1.0  # penalty for illegal\n",
    "            self.done = self.game.is_game_over()\n",
    "\n",
    "        obs = self.encode_observation()\n",
    "        info = {\n",
    "            \"num_pegs\": self.game.board.count_pegs(),\n",
    "            \"is_solved\": self.game.is_solved(),\n",
    "        }\n",
    "        return obs, reward, self.done, info\n",
    "\n",
    "    # ---------------------- helpers ---------------------------- #\n",
    "    def get_legal_actions(self) -> List[Tuple[int, int, int]]:\n",
    "        return self.game.get_legal_actions()\n",
    "\n",
    "    def get_legal_action_mask(\n",
    "        self, action_space_size: int, to_idx: Callable[[Tuple[int, int, int]], int]\n",
    "    ) -> np.ndarray:\n",
    "        mask = np.zeros(action_space_size, dtype=np.float32)\n",
    "        for a in self.get_legal_actions():\n",
    "            mask[to_idx(a)] = 1.0\n",
    "        return mask\n",
    "\n",
    "    # ------------------ observation encoding ------------------- #\n",
    "    def encode_observation(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Channel mapping\n",
    "        0 â€“ peg(1)/hole(0)\n",
    "        1 â€“ fraction removed   (broadcast scalar)\n",
    "        2 â€“ fraction remaining (broadcast scalar)\n",
    "        3 â€“ legal mask\n",
    "        \"\"\"\n",
    "        arr = self.game.board.get_state().astype(np.float32)  # 7Ã—7\n",
    "        num_pegs = arr[arr == 1].size\n",
    "        removed, remain = (self.TOTAL_PEGS - num_pegs) / self.TOTAL_PEGS, num_pegs / self.TOTAL_PEGS\n",
    "        obs = np.zeros((self.BOARD_SIZE, self.BOARD_SIZE, 4), dtype=np.float32)\n",
    "        obs[:, :, 0] = (arr == 1).astype(np.float32)\n",
    "        obs[:, :, 1] = removed\n",
    "        obs[:, :, 2] = remain\n",
    "        obs[:, :, 3] = self.board_mask\n",
    "        return obs\n",
    "\n",
    "    # --------------------- render / debug ---------------------- #\n",
    "    def render(self, mode: str = \"human\") -> None:\n",
    "        print(self.game.board)\n",
    "\n",
    "    # ------------------- cloning for MCTS ---------------------- #\n",
    "    def clone_state(self, state=None) -> \"PegSolitaireEnv\":\n",
    "        \"\"\"Return *deep* copy of env (keeping Board & Game classes intact).\"\"\"\n",
    "        clone = PegSolitaireEnv(self._board_cls, self._game_cls)\n",
    "        clone.reset(state or self.game.board.to_dict())\n",
    "        return clone\n",
    "\n",
    "    # --------------------- data augmentation ------------------- #\n",
    "    @staticmethod\n",
    "    def augment_observation(obs: np.ndarray, mode: str = \"random\") -> np.ndarray:\n",
    "        augs = []\n",
    "        for r in range(4):\n",
    "            rot = np.rot90(obs, k=r, axes=(0, 1))\n",
    "            for flip in (False, True):\n",
    "                augs.append(np.flip(rot, axis=1) if flip else rot)\n",
    "        if mode == \"all\":\n",
    "            return np.stack(augs)\n",
    "        if mode == \"random\":\n",
    "            return augs[np.random.randint(8)]\n",
    "        return obs\n",
    "\n",
    "    # ---------- action augmentation (for symmetry RL) ---------- #\n",
    "    @staticmethod\n",
    "    def augment_action(\n",
    "        action: Tuple[int, int, int], rot: int = 0, flip: bool = False\n",
    "    ) -> Tuple[int, int, int]:\n",
    "        row, col, d = action\n",
    "        directions = [(-2, 0), (2, 0), (0, -2), (0, 2)]\n",
    "        dr, dc = directions[d]\n",
    "        trg = (row + dr, col + dc)\n",
    "\n",
    "        # apply rotation(s)\n",
    "        for _ in range(rot):\n",
    "            row, col = col, 6 - row\n",
    "            trg = (trg[1], 6 - trg[0])\n",
    "        # apply mirror\n",
    "        if flip:\n",
    "            row, col = row, 6 - col\n",
    "            trg = (trg[0], 6 - trg[1])\n",
    "\n",
    "        # recompute dir index\n",
    "        diff = (trg[0] - row, trg[1] - col)\n",
    "        d_new = directions.index(diff)\n",
    "        return row, col, d_new"
   ],
   "id": "8d5312093c51b812"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:25:40.959031Z",
     "start_time": "2025-06-09T05:25:40.941586Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 37,
   "source": [
    "from __future__ import annotations\n",
    "import random\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#                         Neural-Net  (ResNet)                           #\n",
    "# ---------------------------------------------------------------------- #\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = F.relu(self.bn1(self.conv1(x)))\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        return F.relu(x + y)\n",
    "\n",
    "class PegSolitaireNet(nn.Module):\n",
    "    \"\"\"\n",
    "    ×¨×©×ª ×¢×¦×‘×™×ª ×œ×©×—×§×Ÿ ××—×©×‘×ª:\n",
    "      - ×§×œ×˜: (batch, 4, 7, 7)\n",
    "      - ×¨××© Policy: ×”×ª×¤×œ×’×•×ª ×¢×œ ×›×œ ×”×¤×¢×•×œ×•×ª (N_actions)\n",
    "      - ×¨××© Value: ××¡×¤×¨ ×‘×™×Ÿ -1 ×œ-1 (×”×¢×¨×›×ª ×¡×™×›×•×™ ×¤×ª×¨×•×Ÿ)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_actions, n_res_blocks=10, channels=64, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.conv_in = nn.Conv2d(4, channels, 3, padding=1)\n",
    "        self.bn_in = nn.BatchNorm2d(channels)\n",
    "        self.res_blocks = nn.Sequential(*[ResidualBlock(channels) for _ in range(n_res_blocks)])\n",
    "        # Policy Head\n",
    "        self.policy_conv = nn.Conv2d(channels, 4, 1)  # 4 ×›×™ 4 ×›×™×•×•× ×™×\n",
    "        self.policy_bn = nn.BatchNorm2d(4)\n",
    "        self.policy_fc = nn.Linear(4 * 7 * 7, n_actions)\n",
    "        # Value Head\n",
    "        self.value_conv = nn.Conv2d(channels, 2, 1)\n",
    "        self.value_bn = nn.BatchNorm2d(2)\n",
    "        self.value_fc1 = nn.Linear(2 * 7 * 7, 64)\n",
    "        self.value_fc2 = nn.Linear(64, 1)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, 4, 7, 7)\n",
    "        x = F.relu(self.bn_in(self.conv_in(x)))\n",
    "        x = self.res_blocks(x)\n",
    "\n",
    "        # Policy\n",
    "        p = F.relu(self.policy_bn(self.policy_conv(x)))\n",
    "        # FIX THIS LINE\n",
    "        p = p.reshape(x.size(0), -1) # Was: p.view(x.size(0), -1)\n",
    "        p = self.policy_fc(p)\n",
    "\n",
    "        # Value\n",
    "        v = F.relu(self.value_bn(self.value_conv(x)))\n",
    "        # AND FIX THIS LINE\n",
    "        v = v.reshape(x.size(0), -1) # Was: v.view(x.size(0), -1)\n",
    "        v = F.relu(self.value_fc1(v))\n",
    "        v = torch.tanh(self.value_fc2(v))  # ×¢×¨×š ×‘×˜×•×•×— [-1,1]\n",
    "\n",
    "        return p, v.squeeze(-1)\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#                          Replay-Buffer                                 #\n",
    "# ---------------------------------------------------------------------- #\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size replay buffer (FIFO).\"\"\"\n",
    "\n",
    "    def __init__(self, max_size: int = 50_000) -> None:\n",
    "        self._buf: List[Tuple[np.ndarray, np.ndarray, float]] = []\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def push(self, sample: Tuple[np.ndarray, np.ndarray, float]) -> None:\n",
    "        self._buf.append(sample)\n",
    "        if len(self._buf) > self.max_size:\n",
    "            self._buf.pop(0)\n",
    "\n",
    "    def sample(self, batch: int) -> List[Tuple[np.ndarray, np.ndarray, float]]:\n",
    "        return random.sample(self._buf, min(len(self._buf), batch))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._buf)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#                           MCTS (AlphaZero)                             #\n",
    "# ---------------------------------------------------------------------- #\n",
    "class _Node:\n",
    "    __slots__ = (\"prior\", \"children\", \"visit\", \"value_sum\")\n",
    "\n",
    "    def __init__(self, prior: float) -> None:\n",
    "        self.prior = prior\n",
    "        self.children: Dict[int, _Node] = {}\n",
    "        self.visit = 0\n",
    "        self.value_sum = 0.0\n",
    "\n",
    "    @property\n",
    "    def value(self) -> float:\n",
    "        return self.value_sum / self.visit if self.visit else 0.0\n",
    "\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        model: PegSolitaireNet,\n",
    "        action_space,\n",
    "        sims: int = 100,\n",
    "        c_puct: float = 1.5,\n",
    "        device: str | torch.device = \"cpu\",\n",
    "    ) -> None:\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.action_space = action_space\n",
    "        self.sims = sims\n",
    "        self.c_puct = c_puct\n",
    "        self.device = torch.device(device)\n",
    "\n",
    "    # ------------- core -------------- #\n",
    "    def run(self, root_obs: np.ndarray) -> np.ndarray:\n",
    "        root = self._expand(root_obs, self.env.get_legal_actions())\n",
    "        for _ in range(self.sims):\n",
    "            node, env_copy, path = root, self.env.clone_state(), [root]\n",
    "            # selection\n",
    "            while node.children:\n",
    "                act_idx, node = self._select(node)\n",
    "                env_copy.step(self.action_space.from_index(act_idx))\n",
    "                path.append(node)\n",
    "            # expansion + evaluation\n",
    "            obs = env_copy.encode_observation()\n",
    "            node.children = self._expand(obs, env_copy.get_legal_actions()).children\n",
    "            value = self._evaluate(obs)\n",
    "            # back-prop\n",
    "            for n in path:\n",
    "                n.visit += 1\n",
    "                n.value_sum += value\n",
    "        # policy as visit-counts\n",
    "        pi = np.zeros(len(self.action_space), dtype=np.float32)\n",
    "        for idx, child in root.children.items():\n",
    "            pi[idx] = child.visit\n",
    "        return pi / (pi.sum() + 1e-8)\n",
    "\n",
    "    # ---------- helpers ------------- #\n",
    "    def _evaluate(self, obs: np.ndarray) -> float:\n",
    "        with torch.no_grad():\n",
    "            t = torch.tensor(obs).float().permute(2, 0, 1).unsqueeze(0).to(self.device)\n",
    "            _, v = self.model(t)\n",
    "        return v.item()\n",
    "\n",
    "    def _expand(\n",
    "        self, obs: np.ndarray, legal: List[Tuple[int, int, int]]\n",
    "    ) -> _Node:\n",
    "        with torch.no_grad():\n",
    "            t = torch.tensor(obs).float().permute(2, 0, 1).unsqueeze(0).to(self.device)\n",
    "            logits, _ = self.model(t)\n",
    "            probs = torch.softmax(logits, -1).cpu().numpy().flatten()\n",
    "        mask = self.action_space.legal_action_mask(legal)\n",
    "        probs *= mask\n",
    "        if probs.sum() == 0:\n",
    "            probs = mask / mask.sum()\n",
    "        node = _Node(prior=1.0)\n",
    "        for a in legal:\n",
    "            idx = self.action_space.to_index(a)\n",
    "            node.children[idx] = _Node(prior=probs[idx])\n",
    "        return node\n",
    "\n",
    "    def _select(self, node: _Node) -> Tuple[int, _Node]:\n",
    "        total = np.sqrt(node.visit)\n",
    "        best, best_child = -1.0, -1\n",
    "        for idx, child in node.children.items():\n",
    "            u = self.c_puct * child.prior * total / (1 + child.visit)\n",
    "            score = child.value + u\n",
    "            if score > best:\n",
    "                best, best_child = score, idx\n",
    "        return best_child, node.children[best_child]\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#                            Agent                                       #\n",
    "# ---------------------------------------------------------------------- #\n",
    "class Agent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        model: PegSolitaireNet,\n",
    "        action_space,\n",
    "        buffer: ReplayBuffer,\n",
    "        sims: int = 100,\n",
    "        device: str | torch.device = \"cpu\",\n",
    "    ) -> None:\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.action_space = action_space\n",
    "        self.buffer = buffer\n",
    "        self.device = torch.device(device)\n",
    "        self.mcts = MCTS(env, model, action_space, sims, device=self.device)\n",
    "\n",
    "    # ------------------ self-play ------------------ #\n",
    "    def self_play_episode(self, augment: bool = True) -> None:\n",
    "        obs, _ = self.env.reset()\n",
    "        done = False\n",
    "        states, policies = [], []\n",
    "        while not done:\n",
    "            pi = self.mcts.run(obs)\n",
    "            action_idx = np.random.choice(len(pi), p=pi)\n",
    "            action = self.action_space.from_index(action_idx)\n",
    "            states.append(obs)\n",
    "            policies.append(pi)\n",
    "            obs, reward, done, _ = self.env.step(action)\n",
    "        # store\n",
    "        for s, p in zip(states, policies):\n",
    "            if augment:\n",
    "                for aug in self.env.augment_observation(s, \"all\"):\n",
    "                    self.buffer.push((aug, p, reward))\n",
    "            else:\n",
    "                self.buffer.push((s, p, reward))\n",
    "\n",
    "    # ------------------ training ------------------- #\n",
    "    def train(\n",
    "        self,\n",
    "        batch_size: int = 256,\n",
    "        epochs: int = 1,\n",
    "        lr: float = 1e-3,\n",
    "    ) -> None:\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return\n",
    "        opt = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        for _ in range(epochs):\n",
    "            batch = self.buffer.sample(batch_size)\n",
    "            s, p, v = zip(*batch)\n",
    "            s = torch.tensor(np.stack(s)).float().permute(0, 3, 1, 2).contiguous().to(self.device)\n",
    "            p = torch.tensor(np.stack(p)).float().to(self.device)\n",
    "            v = torch.tensor(v).float().to(self.device)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            logits, v_pred = self.model(s)\n",
    "            loss_pol = F.kl_div(\n",
    "                torch.log_softmax(logits, -1), p, reduction=\"batchmean\"\n",
    "            )\n",
    "            loss_val = F.mse_loss(v_pred, v)\n",
    "            loss = loss_pol + loss_val\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "#                       Action-Space helper                              #\n",
    "# ---------------------------------------------------------------------- #\n",
    "class PegSolitaireActionSpace:\n",
    "    \"\"\"Maps (row,col,dir) â†” flat-index and provides legal mask.\"\"\"\n",
    "\n",
    "    def __init__(self, board_mask: np.ndarray) -> None:\n",
    "        self.valid_cells = [\n",
    "            (r, c) for r in range(7) for c in range(7) if board_mask[r, c] == 1\n",
    "        ]\n",
    "        self.actions: List[Tuple[int, int, int]] = [\n",
    "            (r, c, d) for r, c in self.valid_cells for d in range(4)\n",
    "        ]\n",
    "        self._to_idx: Dict[Tuple[int, int, int], int] = {\n",
    "            a: i for i, a in enumerate(self.actions)\n",
    "        }\n",
    "\n",
    "    # mapping\n",
    "    def to_index(self, a: Tuple[int, int, int]) -> int:\n",
    "        return self._to_idx[a]\n",
    "\n",
    "    def from_index(self, idx: int) -> Tuple[int, int, int]:\n",
    "        return self.actions[idx]\n",
    "\n",
    "    # mask\n",
    "    def legal_action_mask(self, legal: List[Tuple[int, int, int]]) -> np.ndarray:\n",
    "        mask = np.zeros(len(self.actions), dtype=np.float32)\n",
    "        for a in legal:\n",
    "            mask[self.to_index(a)] = 1.0\n",
    "        return mask\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.actions)"
   ],
   "id": "c1b8cbb1c899e791"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:25:41.375130Z",
     "start_time": "2025-06-09T05:25:41.271836Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "execution_count": 38,
   "source": [
    "import os, types, tkinter as tk\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ------------------- PATHS / PARAMS ------------------- #\n",
    "AGENT_PATH      = \"peg_agent.pt\"\n",
    "TRAIN_EPISODES  = 800           # ××¤×™×–×•×“×•×ª self-play ×× ××™×Ÿ ×§×•×‘×¥\n",
    "DEVICE          = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# ------------ Runtime patch â€“ fix _expand ------------- #\n",
    "def _safe_expand(self, obs, legal):\n",
    "    \"\"\"Replacement for MCTS._expand â€“ × ×× ×¢ ×—×œ×•×§×” ×‘××¤×¡.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        t = torch.tensor(obs).float().permute(2,0,1).unsqueeze(0).to(self.device)\n",
    "        logits, _ = self.model(t)\n",
    "        probs = torch.softmax(logits, -1).cpu().numpy().flatten()\n",
    "\n",
    "    mask = self.action_space.legal_action_mask(legal)\n",
    "    probs *= mask\n",
    "    if mask.sum() == 0:                       # ××™×Ÿ ××”×œ×›×™× ×—×•×§×™×™×\n",
    "        return _Node(prior=1.0)               # ×¦×•××ª ×¢×œ×”\n",
    "\n",
    "    probs /= mask.sum()                       # × ×•×¨××œ×™×–×¦×™×” ×‘×˜×•×—×”\n",
    "    node = _Node(prior=1.0)\n",
    "    for a in legal:\n",
    "        idx = self.action_space.to_index(a)\n",
    "        node.children[idx] = _Node(prior=float(probs[idx]))\n",
    "    return node\n",
    "\n",
    "# ×”×—×œ×¤×ª ×”××ª×•×“×” ×‘××—×œ×§×”\n",
    "MCTS._expand = _safe_expand            # type: ignore\n",
    "\n",
    "# -------------- Helper save / load -------------------- #\n",
    "def save_agent(agent, path=AGENT_PATH):\n",
    "    torch.save({\n",
    "        \"state_dict\": agent.model.state_dict(),\n",
    "        \"n_actions\" : len(agent.action_space),\n",
    "        \"sims\"      : agent.mcts.sims,\n",
    "    }, path)\n",
    "    print(f\"âœ“ Agent saved âœ {path}\")\n",
    "\n",
    "def load_agent(path=AGENT_PATH):\n",
    "    ckpt  = torch.load(path, map_location=DEVICE)\n",
    "    env   = PegSolitaireEnv(Board, Game)\n",
    "    asp   = PegSolitaireActionSpace(env.board_mask)\n",
    "    model = PegSolitaireNet(ckpt[\"n_actions\"], device=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"state_dict\"]); model.eval()\n",
    "    return Agent(env, model, asp, ReplayBuffer(),\n",
    "                 sims=ckpt.get(\"sims\", 100), device=DEVICE)\n",
    "\n",
    "# -------------- Load or Train ------------------------- #\n",
    "if os.path.exists(AGENT_PATH):\n",
    "    agent = load_agent()\n",
    "else:\n",
    "    env   = PegSolitaireEnv(Board, Game)\n",
    "    asp   = PegSolitaireActionSpace(env.board_mask)\n",
    "    model = PegSolitaireNet(len(asp), device=DEVICE)\n",
    "    buf   = ReplayBuffer()\n",
    "    agent = Agent(env, model, asp, buf, sims=100, device=DEVICE)\n",
    "\n",
    "    print(\"â€¢ Training new agent â€¦\")\n",
    "    for ep in range(1, TRAIN_EPISODES + 1):\n",
    "        agent.self_play_episode()\n",
    "        if ep % 10 == 0 and len(buf) > 1024:\n",
    "            agent.train(batch_size=256, epochs=3, lr=1e-3)\n",
    "        if ep % 100 == 0:\n",
    "            print(f\"  â†³ episode {ep:4d}/{TRAIN_EPISODES} | buffer={len(buf)}\")\n",
    "    save_agent(agent)\n"
   ],
   "id": "85d652ee22409ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:26:14.851775Z",
     "start_time": "2025-06-09T05:25:41.598008Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 39,
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "import numpy as np\n",
    "import torch   # × ×“×¨×© ×œ-value-bar\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------- #\n",
    "class PegSolitaireGUI(tk.Frame):\n",
    "    \"\"\"GUI ×¢× ×¤×¡-×”×¦×œ×—×” ×•-Hint ××”×¡×•×›×Ÿ.\"\"\"\n",
    "\n",
    "    CELL_SIZE, PEG_RADIUS, PADDING = 60, 22, 16\n",
    "    PEG_COLOR, HOLE_COLOR          = \"#FFD600\", \"#202020\"\n",
    "    OUTLINE_COLOR, HIGHLIGHT_COLOR = \"#333\",    \"#42A5F5\"\n",
    "    SUGGEST_COLOR, BG_COLOR        = \"#00C853\", \"#eeeeee\"\n",
    "    BAR_W, BAR_H                   = 160, 16\n",
    "\n",
    "    def __init__(self, master, game: \"Game\", agent=None):\n",
    "        super().__init__(master, bg=self.BG_COLOR)\n",
    "        self.game   = game\n",
    "        self.agent  = agent\n",
    "        self.selected_pos = None            # type: tuple[int,int] | None\n",
    "        self.highlight_move = None          # type: tuple[tuple[int,int],tuple[int,int]]|None\n",
    "\n",
    "        # --- Canvas -------------------------------------------------- #\n",
    "        side = 7 * self.CELL_SIZE + 2 * self.PADDING\n",
    "        self.canvas = tk.Canvas(self, width=side, height=side,\n",
    "                                bg=self.BG_COLOR, highlightthickness=0)\n",
    "        self.canvas.pack()\n",
    "\n",
    "        # --- Status row + Value bar ---------------------------------- #\n",
    "        stat_frm = tk.Frame(self, bg=self.BG_COLOR); stat_frm.pack(pady=4, fill=\"x\")\n",
    "        self.status_label = tk.Label(stat_frm, font=(\"Arial\", 14),\n",
    "                                     bg=self.BG_COLOR, anchor=\"w\")\n",
    "        self.status_label.pack(side=\"left\", padx=4, fill=\"x\", expand=True)\n",
    "        self.bar_canvas = tk.Canvas(stat_frm, width=self.BAR_W, height=self.BAR_H,\n",
    "                                    bg=self.BG_COLOR, highlightthickness=0)\n",
    "        self.bar_canvas.pack(side=\"right\", padx=6)\n",
    "        self._draw_value_bar(0.0)\n",
    "\n",
    "        # --- Buttons ------------------------------------------------- #\n",
    "        btn_frm = tk.Frame(self, bg=self.BG_COLOR); btn_frm.pack()\n",
    "        self.undo_btn  = tk.Button(btn_frm, text=\"â†©ï¸ ×‘×™×˜×•×œ\",  command=self.on_undo)\n",
    "        self.redo_btn  = tk.Button(btn_frm, text=\"â†ªï¸ ×§×“×™××”\",  command=self.on_redo)\n",
    "        self.reset_btn = tk.Button(btn_frm, text=\"××©×—×§ ×—×“×©\", command=self.on_reset)\n",
    "        self.hint_btn  = tk.Button(btn_frm, text=\"ğŸ¤– ×”××œ×¦×”\",  command=self.on_hint)\n",
    "        for col, b in enumerate((self.undo_btn, self.redo_btn,\n",
    "                                 self.reset_btn, self.hint_btn)):\n",
    "            b.grid(row=0, column=col, padx=3)\n",
    "\n",
    "        # --- Move-log ----------------------------------------------- #\n",
    "        tk.Label(self, text=\"××”×œ×›×™×:\", font=(\"Arial\", 12),\n",
    "                 bg=self.BG_COLOR).pack()\n",
    "        self.log_list = tk.Listbox(self, height=6, width=42,\n",
    "                                   font=(\"Consolas\", 11)); self.log_list.pack(pady=(0, 8))\n",
    "\n",
    "        # bindings\n",
    "        self.canvas.bind(\"<Button-1>\", self.on_canvas_click)\n",
    "        master.bind(\"<Control-z>\", lambda e: self.on_undo())\n",
    "        master.bind(\"<Control-y>\", lambda e: self.on_redo())\n",
    "\n",
    "        self.redraw()\n",
    "\n",
    "    # ========================================================= #\n",
    "    #                    ×¦×™×•×¨ ×œ×•×— ×•-Value-Bar                    #\n",
    "    # ========================================================= #\n",
    "    def board_to_xy(self, pos):\n",
    "        r, c = pos\n",
    "        return (self.PADDING + c * self.CELL_SIZE + self.CELL_SIZE // 2,\n",
    "                self.PADDING + r * self.CELL_SIZE + self.CELL_SIZE // 2)\n",
    "\n",
    "    def redraw(self):\n",
    "        self.canvas.delete(\"all\")\n",
    "        # ×¤×™× ×™× / ×—×•×¨×™×\n",
    "        for pos in Board.LEGAL_POSITIONS:\n",
    "            x, y = self.board_to_xy(pos)\n",
    "            fill = self.PEG_COLOR if self.game.board.get(pos) == 1 else self.HOLE_COLOR\n",
    "            width  = 3 if pos == self.selected_pos else 1\n",
    "            outline = self.HIGHLIGHT_COLOR if pos == self.selected_pos else self.OUTLINE_COLOR\n",
    "            self.canvas.create_oval(x-self.PEG_RADIUS, y-self.PEG_RADIUS,\n",
    "                                    x+self.PEG_RADIUS, y+self.PEG_RADIUS,\n",
    "                                    fill=fill, outline=outline, width=width)\n",
    "        # ×™×¢×“×™× ×—×•×§×™×™×\n",
    "        if self.selected_pos:\n",
    "            for dst in [d for s, d, _ in self.game.get_legal_moves() if s == self.selected_pos]:\n",
    "                x, y = self.board_to_xy(dst)\n",
    "                self.canvas.create_oval(x-self.PEG_RADIUS//2, y-self.PEG_RADIUS//2,\n",
    "                                        x+self.PEG_RADIUS//2, y+self.PEG_RADIUS//2,\n",
    "                                        outline=self.HIGHLIGHT_COLOR, width=3)\n",
    "        # ×§×•-×”××œ×¦×”\n",
    "        if self.highlight_move:\n",
    "            src, dst = self.highlight_move\n",
    "            x1, y1 = self.board_to_xy(src)\n",
    "            x2, y2 = self.board_to_xy(dst)\n",
    "            self.canvas.create_line(x1, y1, x2, y2, fill=self.SUGGEST_COLOR,\n",
    "                                    width=5, arrow=tk.LAST)\n",
    "\n",
    "        self.update_status(); self.update_buttons()\n",
    "        self.update_move_log(); self.update_value_bar()\n",
    "\n",
    "    # ------------- Value-Bar helpers ---------------- #\n",
    "    def _draw_value_bar(self, v):\n",
    "        self.bar_canvas.delete(\"all\")\n",
    "        frac   = (v + 1) / 2\n",
    "        length = int(frac * self.BAR_W)\n",
    "        color  = \"#d50000\" if v < -0.3 else \"#9e9e9e\" if v < 0.3 else \"#00c853\"\n",
    "        self.bar_canvas.create_rectangle(0, 0, length, self.BAR_H, fill=color, width=0)\n",
    "        self.bar_canvas.create_rectangle(0, 0, self.BAR_W, self.BAR_H, outline=\"#555\")\n",
    "\n",
    "    def update_value_bar(self):\n",
    "        if self.agent is None:\n",
    "            self._draw_value_bar(0.0); return\n",
    "        obs = self.game.board.encode_observation()\n",
    "        with torch.no_grad():\n",
    "            t = torch.tensor(obs).float().permute(2,0,1).unsqueeze(0).to(self.agent.device)\n",
    "            _, value = self.agent.model(t)\n",
    "        self._draw_value_bar(float(value))\n",
    "\n",
    "    # ========================================================= #\n",
    "    #                   ××™×¨×•×¢×™ ×××©×§                             #\n",
    "    # ========================================================= #\n",
    "    def on_canvas_click(self, e):\n",
    "        pos = ((e.y - self.PADDING)//self.CELL_SIZE,\n",
    "               (e.x - self.PADDING)//self.CELL_SIZE)\n",
    "        if pos not in Board.LEGAL_POSITIONS: return\n",
    "        if self.selected_pos is None and self.game.board.get(pos)==1:\n",
    "            self.selected_pos = pos\n",
    "        elif self.selected_pos and pos != self.selected_pos:\n",
    "            ok, *_ = self.game.apply_move(self.selected_pos, pos)\n",
    "            if ok: self.selected_pos = self.highlight_move = None\n",
    "        else:\n",
    "            self.selected_pos = None\n",
    "        self.redraw()\n",
    "\n",
    "    def _generic(self, fn, msg=\"\"):\n",
    "        if not fn() and msg: self.status_label.config(text=msg)\n",
    "        self.selected_pos = self.highlight_move = None; self.redraw()\n",
    "\n",
    "    def on_undo(self):  self._generic(self.game.undo, \"××™×Ÿ ××”×œ×š ×œ×‘×˜×œ.\")\n",
    "    def on_redo(self):  self._generic(self.game.redo, \"××™×Ÿ ××”×œ×š ×œ×©×—×–×¨.\")\n",
    "    def on_reset(self): self._generic(self.game.reset)\n",
    "\n",
    "    def on_hint(self):\n",
    "        if self.agent is None:\n",
    "            messagebox.showinfo(\"×”××œ×¦×”\", \"××™×Ÿ ×¡×•×›×Ÿ ××—×•×‘×¨.\"); return\n",
    "        obs = self.game.board.encode_observation()\n",
    "        pi  = self.agent.mcts.run(obs)\n",
    "        idx = int(np.argmax(pi))\n",
    "        r,c,d = self.agent.action_space.from_index(idx)\n",
    "        dr,dc = Game.DIRECTIONS[d]\n",
    "        self.highlight_move = ((r,c), (r+dr, c+dc))\n",
    "        self.status_label.config(text=f\"×”××œ×¦×”: {(r,c)} â†  {(r+dr,c+dc)}\")\n",
    "        self.redraw()\n",
    "\n",
    "    # ----------------- UI helpers ------------------ #\n",
    "    def update_status(self):\n",
    "        if self.game.is_win():\n",
    "            text=\"× ×™×¦×—×•×Ÿ! ×¤×™×•×Ÿ ×™×—×™×“ ×‘××¨×›×– ğŸ‘‘\"\n",
    "        elif self.game.is_game_over():\n",
    "            text=f\"×¡×™×•× â€¢ ××™×Ÿ ××”×œ×›×™× ({len(self.game.move_log)})\"\n",
    "        else:\n",
    "            text=f\"×¤×™× ×™×: {self.game.board.count_pegs()} | ××”×œ×š: {len(self.game.move_log)}\"\n",
    "        self.status_label.config(text=text)\n",
    "\n",
    "    def update_buttons(self):\n",
    "        self.undo_btn.config(state=tk.NORMAL if self.game.move_history else tk.DISABLED)\n",
    "        self.redo_btn.config(state=tk.NORMAL if self.game.redo_stack   else tk.DISABLED)\n",
    "\n",
    "    def update_move_log(self):\n",
    "        self.log_list.delete(0, tk.END)\n",
    "        for i,(src,over,dst) in enumerate(self.game.move_log,1):\n",
    "            self.log_list.insert(tk.END, f\"{i:2}: {src} â†’ {dst} (/{over})\")\n",
    "\n",
    "\n",
    "# ---------------- DEMO (××©×ª××© ×‘-agent ×× ×§×™×™×) --------------- #\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk(); root.title(\"××—×©×‘×ª â€“ Peg-Solitaire\")\n",
    "\n",
    "    try:         # ×× ×‘××•×“×•×œ ×”×¨××©×™ ×›×‘×¨ ×§×™×™× agent ×××•××Ÿ â€“ × ×©×ª××© ×‘×•\n",
    "        agent  # type: ignore\n",
    "    except NameError:\n",
    "        class DummyAgent:           # ×’×™×‘×•×™\n",
    "            device=torch.device(\"cpu\")\n",
    "            action_space=None\n",
    "            def mcts(self): ...\n",
    "            class model:            # noqa\n",
    "                @staticmethod\n",
    "                def __call__(*_,**__): return None, torch.tensor([0.0])\n",
    "        agent = DummyAgent()        # type: ignore\n",
    "\n",
    "    gui = PegSolitaireGUI(root, Game(), agent=agent)\n",
    "    gui.pack(); root.mainloop()"
   ],
   "id": "664ca3863ac27af7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d3fcb4c0ebdc7fb7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
